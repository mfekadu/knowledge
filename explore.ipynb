{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/bin/pip\n",
      "\n",
      "/usr/bin/python\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/bin/python3\n",
      "\n",
      "/bin/sh: /Library/Frameworks/Python.framework/Versions/3.7/bin/spacy: Permission denied\n"
     ]
    }
   ],
   "source": [
    "!which pip # expect pip to be inside .../Versions/3.x/bin/pip\n",
    "!echo\n",
    "!which python\n",
    "!echo\n",
    "!which python3 # make sure python3 is a thing\n",
    "!echo\n",
    "!spacy # make sure spacy is a thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.1.6)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.16.3)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (7.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.32.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "BABEL_API_KEY = \"<INSERT_YOUR_API_KEY>\" # https://babelnet.org/login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR:\t ['words.csv', 'speeches', 'README.md', 'index.csv']\n",
      "SPEECH_DIR[:2]:\t ['Minnesota_SOTS.txt', 'Indiana_SOTS.txt']\n",
      "TABLE_PATH:\t state-of-the-state/index.csv\n",
      "SPEECH_DIR:\t state-of-the-state/speeches\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"state-of-the-state\"\n",
    "FILENAME = \"index.csv\"\n",
    "SPEECH_DIR_NAME = \"speeches\"\n",
    "TABLE_PATH = os.path.join(DATA_DIR, FILENAME)\n",
    "SPEECH_DIR = os.path.join(DATA_DIR, SPEECH_DIR_NAME)\n",
    "print(\"DATA_DIR:\\t\", os.listdir(DATA_DIR))\n",
    "print(\"SPEECH_DIR[:2]:\\t\", os.listdir(SPEECH_DIR)[:2])\n",
    "print(\"TABLE_PATH:\\t\", TABLE_PATH)\n",
    "print(\"SPEECH_DIR:\\t\", SPEECH_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>governor</th>\n",
       "      <th>party</th>\n",
       "      <th>filename</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Kay Ivey</td>\n",
       "      <td>R</td>\n",
       "      <td>Alabama_SOTS.txt</td>\n",
       "      <td>https://governor.alabama.gov/remarks-speeches/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Mike Dunleavy</td>\n",
       "      <td>R</td>\n",
       "      <td>Alaska_SOTS.txt</td>\n",
       "      <td>https://gov.alaska.gov/newsroom/2019/01/22/201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Doug Ducey</td>\n",
       "      <td>R</td>\n",
       "      <td>Arizona_SOTS.txt</td>\n",
       "      <td>https://azgovernor.gov/governor/news/2019/01/g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Asa Hutchinson</td>\n",
       "      <td>R</td>\n",
       "      <td>Arkansas_SOTS.txt</td>\n",
       "      <td>https://governor.arkansas.gov/news-media/speec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>Gavin Newsom</td>\n",
       "      <td>D</td>\n",
       "      <td>California_SOTS.txt</td>\n",
       "      <td>https://www.gov.ca.gov/2019/02/12/state-of-the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state        governor party             filename  \\\n",
       "0     Alabama        Kay Ivey     R     Alabama_SOTS.txt   \n",
       "1      Alaska   Mike Dunleavy     R      Alaska_SOTS.txt   \n",
       "2     Arizona      Doug Ducey     R     Arizona_SOTS.txt   \n",
       "3    Arkansas  Asa Hutchinson     R    Arkansas_SOTS.txt   \n",
       "4  California    Gavin Newsom     D  California_SOTS.txt   \n",
       "\n",
       "                                                 url  \n",
       "0  https://governor.alabama.gov/remarks-speeches/...  \n",
       "1  https://gov.alaska.gov/newsroom/2019/01/22/201...  \n",
       "2  https://azgovernor.gov/governor/news/2019/01/g...  \n",
       "3  https://governor.arkansas.gov/news-media/speec...  \n",
       "4  https://www.gov.ca.gov/2019/02/12/state-of-the...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv for info on the data\n",
    "df = pd.read_csv(TABLE_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping filenames to text content\n",
    "speech_map = {}\n",
    "\n",
    "# read the files\n",
    "for fname in df['filename'].get_values():\n",
    "    with open( os.path.join(SPEECH_DIR, fname) ) as f:\n",
    "        speech_map[fname] = f.read()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/api/span#set_extension\n",
    "# extend Span object with is_stop\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# if the length is 1 and the word is a stopword\n",
    "stopword_checker_for_span = lambda span: (len(span) == 1 and type(span[0]).__name__ == \"Token\" and span[0].is_stop)\n",
    "Span.set_extension(\"is_stop\", getter=stopword_checker_for_span)\n",
    "\n",
    "# test ._.is_stop\n",
    "assert(nlp(\"I\")[:]._.is_stop == True)\n",
    "assert(nlp(\"Hello world\")[:]._.is_stop == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t...found weird chunk {{ ’s roadways }} ...fixed... [[ roadways ]]\n",
      "\t...found weird chunk {{ ’s youngest learners }} ...fixed... [[ youngest learners ]]\n",
      "\t...found weird chunk {{ ’s infrastructure }} ...fixed... [[ infrastructure ]]\n",
      "got noun chunks for Alabama_SOTS.txt\n",
      "\t...found weird chunk {{ ’s history }} ...fixed... [[ history ]]\n",
      "\t...found weird chunk {{ ’s future }} ...fixed... [[ future ]]\n",
      "got noun chunks for Alaska_SOTS.txt\n",
      "\t...found weird chunk {{ ’s time }} ...fixed... [[ time ]]\n",
      "\t...found weird chunk {{ ’s resolution }} ...fixed... [[ resolution ]]\n",
      "got noun chunks for Arizona_SOTS.txt\n",
      "\t...found weird chunk {{ ’s remarks }} ...fixed... [[ remarks ]]\n",
      "\t...found weird chunk {{ ’s desk motto }} ...fixed... [[ desk motto ]]\n",
      "got noun chunks for Arkansas_SOTS.txt\n",
      "\t...found weird chunk {{ ’s economy }} ...fixed... [[ economy ]]\n",
      "\t...found weird chunk {{ ’s prosperity }} ...fixed... [[ prosperity ]]\n",
      "\t...found weird chunk {{ ’s excellence }} ...fixed... [[ excellence ]]\n",
      "\t...found weird chunk {{ ’s Covered California premiums }} ...fixed... [[ Covered California premiums ]]\n",
      "got noun chunks for California_SOTS.txt\n",
      "\t...found weird chunk {{ ’s safety }} ...fixed... [[ safety ]]\n",
      "\t...found weird chunk {{ ’s farmers }} ...fixed... [[ farmers ]]\n",
      "\t...found weird chunk {{ ’s Co-Op Development Center }} ...fixed... [[ Co-Op Development Center ]]\n",
      "\t...found weird chunk {{ ’s energy workforce }} ...fixed... [[ energy workforce ]]\n",
      "\t...found weird chunk {{ ’s realities }} ...fixed... [[ realities ]]\n",
      "\t...found weird chunk {{ ’s distortions }} ...fixed... [[ distortions ]]\n",
      "got noun chunks for Colorado_SOTS.txt\n",
      "\t...found weird chunk {{ ’s first submarine }} ...fixed... [[ first submarine ]]\n",
      "\t...found weird chunk {{ ’s time }} ...fixed... [[ time ]]\n",
      "\t...found weird chunk {{ ’s end }} ...fixed... [[ end ]]\n",
      "\t...found weird chunk {{ ’s time }} ...fixed... [[ time ]]\n",
      "got noun chunks for Connecticut_SOTS.txt\n",
      "\t...found weird chunk {{ ’s world }} ...fixed... [[ world ]]\n",
      "\t...found weird chunk {{ ’s history }} ...fixed... [[ history ]]\n",
      "\t...found weird chunk {{ ’s interest }} ...fixed... [[ interest ]]\n",
      "got noun chunks for Delaware_SOTS.txt\n",
      "\t...found weird chunk {{ ’s credit }} ...fixed... [[ credit ]]\n",
      "got noun chunks for Florida_SOTS.txt\n",
      "\t...found weird chunk {{ ’s oldest and largest industry }} ...fixed... [[ oldest and largest industry ]]\n",
      "\t...found weird chunk {{ ’s time }} ...fixed... [[ time ]]\n",
      "got noun chunks for Georgia_SOTS.txt\n",
      "\t...found weird chunk {{ ’s future }} ...fixed... [[ future ]]\n",
      "\t...found weird chunk {{ ’s sake }} ...fixed... [[ sake ]]\n",
      "\t...found weird chunk {{ ’s capital }} ...fixed... [[ capital ]]\n",
      "\t...found weird chunk {{ ’s health }} ...fixed... [[ health ]]\n",
      "\t...found weird chunk {{ ’s overall elementary curriculum }} ...fixed... [[ overall elementary curriculum ]]\n",
      "\t...found weird chunk {{ ’s budget }} ...fixed... [[ budget ]]\n",
      "\t...found weird chunk {{ ’s budget }} ...fixed... [[ budget ]]\n",
      "\t...found weird chunk {{ ’s first Local 2030 sustainability }} ...fixed... [[ first Local 2030 sustainability ]]\n",
      "got noun chunks for Hawaii_SOTS.txt\n",
      "\t...found weird chunk {{ ’s history }} ...fixed... [[ history ]]\n",
      "\t...found weird chunk {{ ’s economy }} ...fixed... [[ economy ]]\n",
      "\t...found weird chunk {{ ’s Future }} ...fixed... [[ Future ]]\n",
      "got noun chunks for Idaho_SOTS.txt\n",
      "\t...found weird chunk {{ ’s lives }} ...fixed... [[ lives ]]\n",
      "\t...found weird chunk {{ ’s supply chain hub }} ...fixed... [[ supply chain hub ]]\n",
      "\t...found weird chunk {{ ’s best Chief Marketing Officer }} ...fixed... [[ best Chief Marketing Officer ]]\n",
      "\t...found weird chunk {{ ’s symbol }} ...fixed... [[ symbol ]]\n",
      "got noun chunks for Illinois_Both.txt\n",
      "got noun chunks for Indiana_SOTS.txt\n",
      "\t...found weird chunk {{ ’s future }} ...fixed... [[ future ]]\n",
      "\t...found weird chunk {{ ’s advocates }} ...fixed... [[ advocates ]]\n",
      "\t...found weird chunk {{ ’s system }} ...fixed... [[ system ]]\n",
      "got noun chunks for Iowa_SOTS.txt\n",
      "\t...found weird chunk {{ ’s hospitals }} ...fixed... [[ hospitals ]]\n",
      "\t...found weird chunk {{ ’s Medicaid program }} ...fixed... [[ Medicaid program ]]\n",
      "\t...found weird chunk {{ ’s Task Force }} ...fixed... [[ Task Force ]]\n",
      "\t...found weird chunk {{ ’s Initiatives Fund }} ...fixed... [[ Initiatives Fund ]]\n",
      "got noun chunks for Kansas_SOTS.txt\n",
      "got noun chunks for Kentucky_SOTS.txt\n",
      "\t...found weird chunk {{ ’s history }} ...fixed... [[ history ]]\n",
      "\t...found weird chunk {{ ’s time }} ...fixed... [[ time ]]\n",
      "\t...found weird chunk {{ ’s finances }} ...fixed... [[ finances ]]\n",
      "got noun chunks for Louisiana_SOTS.txt\n",
      "\t...found weird chunk {{ ’s share }} ...fixed... [[ share ]]\n",
      "\t...found weird chunk {{ ’s Office }} ...fixed... [[ Office ]]\n",
      "got noun chunks for Maine_SOTS.txt\n",
      "\t...found weird chunk {{ ’s priorities }} ...fixed... [[ priorities ]]\n",
      "\t...found weird chunk {{ ’s funding formulas }} ...fixed... [[ funding formulas ]]\n",
      "\t...found weird chunk {{ ’s priorities }} ...fixed... [[ priorities ]]\n",
      "got noun chunks for Maryland_SOTS.txt\n",
      "\t...found weird chunk {{ ’s Home }} ...fixed... [[ Home ]]\n",
      "\t...found weird chunk {{ ’s faith }} ...fixed... [[ faith ]]\n",
      "\t...found weird chunk {{ ’s life }} ...fixed... [[ life ]]\n",
      "\t...found weird chunk {{ ’s people }} ...fixed... [[ people ]]\n",
      "\t...found weird chunk {{ ’s public service }} ...fixed... [[ public service ]]\n",
      "got noun chunks for Massachusetts_Both.txt\n",
      "\t...found weird chunk {{ ’s brother }} ...fixed... [[ brother ]]\n",
      "\t...found weird chunk {{ ’s time }} ...fixed... [[ time ]]\n",
      "\t...found weird chunk {{ ’s jobs }} ...fixed... [[ jobs ]]\n",
      "\t...found weird chunk {{ ’s spouse }} ...fixed... [[ spouse ]]\n",
      "got noun chunks for Michigan_SOTS.txt\n",
      "got noun chunks for Minnesota_SOTS.txt\n",
      "\t...found weird chunk {{ ’s history }} ...fixed... [[ history ]]\n",
      "\t...found weird chunk {{ ’s history }} ...fixed... [[ history ]]\n",
      "\t...found weird chunk {{ ’s address }} ...fixed... [[ address ]]\n",
      "got noun chunks for Mississippi_SOTS.txt\n",
      "\t...found weird chunk {{ ’s world }} ...fixed... [[ world ]]\n",
      "\t...found weird chunk {{ ’s health }} ...fixed... [[ health ]]\n",
      "got noun chunks for Missouri_SOTS.txt\n",
      "\t...found weird chunk {{ ’s house }} ...fixed... [[ house ]]\n",
      "\t...found weird chunk {{ ’s history }} ...fixed... [[ history ]]\n",
      "\t...found weird chunk {{ ’s economy }} ...fixed... [[ economy ]]\n",
      "got noun chunks for Montana_SOTS.txt\n",
      "\t...found weird chunk {{ ’s tax incentives }} ...fixed... [[ tax incentives ]]\n",
      "got noun chunks for Nebraska_SOTS.txt\n",
      "\t...found weird chunk {{ ’s Zoom Schools }} ...fixed... [[ Zoom Schools ]]\n",
      "got noun chunks for Nevada_SOTS.txt\n",
      "got noun chunks for NewHampshire_Both.txt\n",
      "got noun chunks for NewJersey_SOTS.txt\n",
      "\t...found weird chunk {{ ’s business }} ...fixed... [[ business ]]\n",
      "\t...found weird chunk {{ ’s order }} ...fixed... [[ order ]]\n",
      "\t...found weird chunk {{ ’s fortunes }} ...fixed... [[ fortunes ]]\n",
      "got noun chunks for NewMexico_SOTS.txt\n",
      "got noun chunks for NewYork_SOTS.txt\n",
      "got noun chunks for NorthCarolina_SOTS.txt\n",
      "\t...found weird chunk {{ ’s America’s Health Rankings }} ...fixed... [[ America’s Health Rankings ]]\n",
      "\t...found weird chunk {{ ’s No. 2 oil producer }} ...fixed... [[ No. 2 oil producer ]]\n",
      "\t...found weird chunk {{ ’s bond rating }} ...fixed... [[ bond rating ]]\n",
      "got noun chunks for NorthDakota_SOTS.txt\n",
      "\t...found weird chunk {{ ’s house }} ...fixed... [[ house ]]\n",
      "got noun chunks for Ohio_SOTS.txt\n",
      "\t...found weird chunk {{ ’s “turnaround }} ...fixed... [[ “turnaround ]]\n",
      "\t...found weird chunk {{ ’s support }} ...fixed... [[ support ]]\n",
      "\t...found weird chunk {{ ’s largest agencies }} ...fixed... [[ largest agencies ]]\n",
      "\t...found weird chunk {{ ’s budget }} ...fixed... [[ budget ]]\n",
      "\t...found weird chunk {{ ’s Mansion }} ...fixed... [[ Mansion ]]\n",
      "\t...found weird chunk {{ ’s tax increases }} ...fixed... [[ tax increases ]]\n",
      "got noun chunks for Oklahoma_SOTS.txt\n",
      "got noun chunks for Oregon_Both.txt\n",
      "\t...found weird chunk {{ ’s time }} ...fixed... [[ time ]]\n",
      "got noun chunks for Pennsylvania_SOTS.txt\n",
      "got noun chunks for RhodeIsland_SOTS.txt\n",
      "\t...found weird chunk {{ ’s Sentinel }} ...fixed... [[ Sentinel ]]\n",
      "\t...found weird chunk {{ ’s marginal income }} ...fixed... [[ marginal income ]]\n",
      "\t...found weird chunk {{ ’s tax structure }} ...fixed... [[ tax structure ]]\n",
      "\t...found weird chunk {{ ’s businesses }} ...fixed... [[ businesses ]]\n",
      "\t...found weird chunk {{ ’s employers }} ...fixed... [[ employers ]]\n",
      "\t...found weird chunk {{ ’s men }} ...fixed... [[ men ]]\n",
      "\t...found weird chunk {{ ’s decisions }} ...fixed... [[ decisions ]]\n",
      "\t...found weird chunk {{ ’s deputies }} ...fixed... [[ deputies ]]\n",
      "\t...found weird chunk {{ ’s modern workplace }} ...fixed... [[ modern workplace ]]\n",
      "\t...found weird chunk {{ ’s technical colleges }} ...fixed... [[ technical colleges ]]\n",
      "got noun chunks for SouthCarolina_SOTS.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t...found weird chunk {{ ’s commitment }} ...fixed... [[ commitment ]]\n",
      "\t...found weird chunk {{ ’s birds }} ...fixed... [[ birds ]]\n",
      "\t...found weird chunk {{ ’s businesses }} ...fixed... [[ businesses ]]\n",
      "\t...found weird chunk {{ ’s decisions }} ...fixed... [[ decisions ]]\n",
      "\t...found weird chunk {{ ’s institutions }} ...fixed... [[ institutions ]]\n",
      "\t...found weird chunk {{ ’s failure }} ...fixed... [[ failure ]]\n",
      "got noun chunks for SouthDakota_SOTS.txt\n",
      "\t...found weird chunk {{ ’s constitution }} ...fixed... [[ constitution ]]\n",
      "\t...found weird chunk {{ ’s concern }} ...fixed... [[ concern ]]\n",
      "\t...found weird chunk {{ ’s Capital }} ...fixed... [[ Capital ]]\n",
      "\t...found weird chunk {{ ’s Services }} ...fixed... [[ Services ]]\n",
      "\t...found weird chunk {{ ’s Excellence }} ...fixed... [[ Excellence ]]\n",
      "got noun chunks for Tennessee_SOTS.txt\n",
      "got noun chunks for Texas_SOTS.txt\n",
      "\t...found weird chunk {{ ’s challenges }} ...fixed... [[ challenges ]]\n",
      "\t...found weird chunk {{ ’s State }} ...fixed... [[ State ]]\n",
      "\t...found weird chunk {{ ’s State }} ...fixed... [[ State ]]\n",
      "\t...found weird chunk {{ ’s State }} ...fixed... [[ State ]]\n",
      "\t...found weird chunk {{ ’s fascination }} ...fixed... [[ fascination ]]\n",
      "\t...found weird chunk {{ ’s education }} ...fixed... [[ education ]]\n",
      "\t...found weird chunk {{ ’s outcasts }} ...fixed... [[ outcasts ]]\n",
      "got noun chunks for Utah_SOTS.txt\n",
      "got noun chunks for Vermont_Both.txt\n",
      "\t...found weird chunk {{ ’s history }} ...fixed... [[ history ]]\n",
      "\t...found weird chunk {{ ’s first-ever Chief School Readiness Officer }} ...fixed... [[ first-ever Chief School Readiness Officer ]]\n",
      "\t...found weird chunk {{ ’s story }} ...fixed... [[ story ]]\n",
      "\t...found weird chunk {{ ’s more deaths }} ...fixed... [[ more deaths ]]\n",
      "\t...found weird chunk {{ ’s time }} ...fixed... [[ time ]]\n",
      "\t...found weird chunk {{ ’s work }} ...fixed... [[ work ]]\n",
      "got noun chunks for Virginia_SOTS.txt\n",
      "\t...found weird chunk {{ ’s problem }} ...fixed... [[ problem ]]\n",
      "\t...found weird chunk {{ ’s life }} ...fixed... [[ life ]]\n",
      "got noun chunks for Washington_SOTS.txt\n",
      "got noun chunks for WestVirginia_SOTS.txt\n",
      "\t...found weird chunk {{ ’s Wellness }} ...fixed... [[ Wellness ]]\n",
      "\t...found weird chunk {{ ’s commitment }} ...fixed... [[ commitment ]]\n",
      "\t...found weird chunk {{ ’s achievement gap }} ...fixed... [[ achievement gap ]]\n",
      "\t...found weird chunk {{ ’s highest-need districts }} ...fixed... [[ highest-need districts ]]\n",
      "got noun chunks for Wisconsin_SOTS.txt\n",
      "\t...found weird chunk {{ ’s history }} ...fixed... [[ history ]]\n",
      "\t...found weird chunk {{ ’s revenue }} ...fixed... [[ revenue ]]\n",
      "\t...found weird chunk {{ ’s support }} ...fixed... [[ support ]]\n",
      "\t...found weird chunk {{ ’s fundamental missions }} ...fixed... [[ fundamental missions ]]\n",
      "\t...found weird chunk {{ ’s weed and pest\n",
      "districts }} ...fixed... [[ weed and pest\n",
      "districts ]]\n",
      "\t...found weird chunk {{ ’s energy problems }} ...fixed... [[ energy problems ]]\n",
      "\t...found weird chunk {{ ’s offices }} ...fixed... [[ offices ]]\n",
      "\t...found weird chunk {{ ’s values }} ...fixed... [[ values ]]\n",
      "got noun chunks for Wyoming_SOTS.txt\n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/usage/linguistic-features#noun-chunks\n",
    "\n",
    "speech_noun_map = {}\n",
    "\n",
    "for state in speech_map:\n",
    "    doc = nlp(speech_map[state])\n",
    "\n",
    "    nouns = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if (chunk._.is_stop): # e.g. if chunk == [\"it\"]\n",
    "            if DEBUG: \n",
    "                print(\"stopword?\", chunk[0], chunk.text)\n",
    "            continue\n",
    "\n",
    "        if (chunk[0].text == \"’s\" or chunk[0].text == \"'s\"):\n",
    "            if len(chunk) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                print(\"\\t...found weird chunk {{\", chunk, \"}} \", end=\"\")\n",
    "                chunk = chunk[1:]\n",
    "                print(f\"...fixed... [[ {chunk} ]]\")\n",
    "\n",
    "        nouns.append(chunk.text)\n",
    "\n",
    "    print(f\"got noun chunks for {state}\")\n",
    "    speech_noun_map[state] = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lieutenant Governor Ainsworth', 'Pro Tempore Marsh', 'Speaker McCutcheon', 'Speaker Pro Tempore Gaston', 'members', 'the Alabama Legislature', 'Chief Justice Parker', 'justices', 'the Alabama Supreme Court', 'my fellow Alabamians', 'Mother Nature', 'the form', 'our state', 'significant devastation', 'At least 23 innocent lives', 'Young children', 'life', 'Mothers', 'Fathers', 'Friends', 'neighbors', 'times', 'the good Lord', 'His continued comfort', 'healing hands', 'special thanks', 'the emergency responders', 'local law enforcement', 'a moment', 'silence', 'many others', 'uncertainty', 'what tomorrow', 'absolute certainty', 'the resiliency', 'the people', 'Alabama', 'a time', 'our entire nation', 'these good people', 'Lee County', 'its feet', 'our 200 years', 'statehood', 'the men', 'women', 'Alabama', 'our nation', 'our country', 'defending']\n"
     ]
    }
   ],
   "source": [
    "print(speech_noun_map['Alabama_SOTS.txt'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_babelnet(chunk):\n",
    "    \"\"\"\n",
    "    given a chunk of words, do call to BabelNet API and get results\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def save_babelnet_results(babel_res, filename):\n",
    "    \"\"\"\n",
    "    given BabelNet result, save to file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open( filename, 'a' ) as f:\n",
    "            f.write(babel_res)\n",
    "            f.close()\n",
    "    except:\n",
    "        with open( filename, 'w' ) as f:\n",
    "            f.write(babel_res)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_babelnet_results(\"lol\\n\", 'lol.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for state in speech_noun_map:\n",
    "    nouns = speech_noun_map[state]\n",
    "    i+=1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
