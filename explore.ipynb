{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/bin/pip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!which pip # expect pip to be inside .../Versions/3.x/bin/pip\n",
    "!echo\n",
    "!which python\n",
    "!echo\n",
    "!which python3 # make sure python3 is a thing\n",
    "!echo\n",
    "!spacy # make sure spacy is a thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "try:\n",
    "    # make a file called .api_key with your API_KEY from BabelNet\n",
    "    with open(\".api_key\") as f: # https://babelnet.org/login\n",
    "        BABEL_API_KEY = f.read()\n",
    "        f.close()\n",
    "except:\n",
    "    # or just for one time use, insert it here\n",
    "    BABEL_API_KEY = \"<INSERT_YOUR_API_KEY>\" # https://babelnet.org/login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"state-of-the-state\"\n",
    "FILENAME = \"index.csv\"\n",
    "SPEECH_DIR_NAME = \"speeches\"\n",
    "TABLE_PATH = os.path.join(DATA_DIR, FILENAME)\n",
    "SPEECH_DIR = os.path.join(DATA_DIR, SPEECH_DIR_NAME)\n",
    "print(\"DATA_DIR:\\t\", os.listdir(DATA_DIR))\n",
    "print(\"SPEECH_DIR[:2]:\\t\", os.listdir(SPEECH_DIR)[:2])\n",
    "print(\"TABLE_PATH:\\t\", TABLE_PATH)\n",
    "print(\"SPEECH_DIR:\\t\", SPEECH_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv for info on the data\n",
    "df = pd.read_csv(TABLE_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping filenames to text content\n",
    "speech_map = {}\n",
    "\n",
    "# read the files\n",
    "for fname in df['filename'].get_values():\n",
    "    with open( os.path.join(SPEECH_DIR, fname) ) as f:\n",
    "        speech_map[fname] = f.read()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/api/span#set_extension\n",
    "# extend Span object with is_stop\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# if the length is 1 and the word is a stopword\n",
    "stopword_checker_for_span = lambda span: (len(span) == 1 and type(span[0]).__name__ == \"Token\" and span[0].is_stop)\n",
    "Span.set_extension(\"is_stop\", getter=stopword_checker_for_span)\n",
    "\n",
    "# test ._.is_stop\n",
    "assert(nlp(\"I\")[:]._.is_stop == True)\n",
    "assert(nlp(\"Hello world\")[:]._.is_stop == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/linguistic-features#noun-chunks\n",
    "\n",
    "speech_noun_map = {}\n",
    "\n",
    "for state in speech_map:\n",
    "    doc = nlp(speech_map[state])\n",
    "\n",
    "    nouns = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if (chunk._.is_stop): # e.g. if chunk == [\"it\"]\n",
    "            if DEBUG: \n",
    "                print(\"stopword?\", chunk[0], chunk.text)\n",
    "            continue\n",
    "\n",
    "        if (chunk[0].text == \"â€™s\" or chunk[0].text == \"'s\"):\n",
    "            if len(chunk) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                print(\"\\t...found weird chunk {{\", chunk, \"}} \", end=\"\")\n",
    "                chunk = chunk[1:]\n",
    "                print(f\"...fixed... [[ {chunk} ]]\")\n",
    "\n",
    "        nouns.append(chunk.text)\n",
    "\n",
    "    print(f\"got noun chunks for {state}\")\n",
    "    speech_noun_map[state] = nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(speech_noun_map['Alabama_SOTS.txt'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_babelnet(chunk):\n",
    "    \"\"\"\n",
    "    given a chunk of words, do call to BabelNet API and get results\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def save_babelnet_results(babel_res, filename):\n",
    "    \"\"\"\n",
    "    given BabelNet result, save to file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open( filename, 'a' ) as f:\n",
    "            f.write(babel_res)\n",
    "            f.close()\n",
    "    except:\n",
    "        with open( filename, 'w' ) as f:\n",
    "            f.write(babel_res)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_babelnet_results(\"lol\\n\", 'lol.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for state in speech_noun_map:\n",
    "    nouns = speech_noun_map[state]\n",
    "    i+=1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
