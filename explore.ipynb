{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### uncomment the following code to set up your computer with the tools\n",
    "# !which pip # expect pip to be inside .../Versions/3.x/bin/pip\n",
    "# !echo\n",
    "# !which python\n",
    "# !echo\n",
    "# !which python3 # make sure python3 is a thing\n",
    "# !echo\n",
    "# !spacy # make sure spacy is a thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BABEL_RESPONSE_DIR[:2]:\t ['Alabama Supreme Court_2019-07-21.gzip', '.DS_Store'] \n",
      "\n",
      "DATA_DIR:\t ['.DS_Store', 'words.csv', 'speeches', 'README.md', 'index.csv']\n",
      "SPEECH_DIR[:2]:\t ['Minnesota_SOTS.txt', 'Indiana_SOTS.txt']\n",
      "TABLE_PATH:\t state-of-the-state/index.csv\n",
      "SPEECH_DIR:\t state-of-the-state/speeches\n",
      "\n",
      "2019-07-22\n",
      "2019-07-22 00:00:49.849451\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import spacy\n",
    "import json\n",
    "import datetime\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.parse import urlencode\n",
    "import gzip\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "SERVICE_URL = 'https://babelnet.io/v5/getSynsetIds'\n",
    "BABEL_RESPONSE_DIR = \"babel_responses\"\n",
    "print(\"BABEL_RESPONSE_DIR[:2]:\\t\", os.listdir(BABEL_RESPONSE_DIR)[:2],\"\\n\")\n",
    "\n",
    "DATA_DIR = \"state-of-the-state\"\n",
    "FILENAME = \"index.csv\"\n",
    "SPEECH_DIR_NAME = \"speeches\"\n",
    "TABLE_PATH = os.path.join(DATA_DIR, FILENAME)\n",
    "SPEECH_DIR = os.path.join(DATA_DIR, SPEECH_DIR_NAME)\n",
    "print(\"DATA_DIR:\\t\", os.listdir(DATA_DIR))\n",
    "print(\"SPEECH_DIR[:2]:\\t\", os.listdir(SPEECH_DIR)[:2])\n",
    "print(\"TABLE_PATH:\\t\", TABLE_PATH)\n",
    "print(\"SPEECH_DIR:\\t\", SPEECH_DIR)\n",
    "print()\n",
    "print(datetime.date.today())\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # make a file called .api_key with your API_KEY from BabelNet\n",
    "    with open(\".api_key\") as f: # https://babelnet.org/login\n",
    "        BABEL_API_KEY = f.read()\n",
    "        f.close()\n",
    "except:\n",
    "    # or just for one time use, insert it here\n",
    "    BABEL_API_KEY = \"<INSERT_YOUR_API_KEY>\" # https://babelnet.org/login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>governor</th>\n",
       "      <th>party</th>\n",
       "      <th>filename</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Kay Ivey</td>\n",
       "      <td>R</td>\n",
       "      <td>Alabama_SOTS.txt</td>\n",
       "      <td>https://governor.alabama.gov/remarks-speeches/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Mike Dunleavy</td>\n",
       "      <td>R</td>\n",
       "      <td>Alaska_SOTS.txt</td>\n",
       "      <td>https://gov.alaska.gov/newsroom/2019/01/22/201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Doug Ducey</td>\n",
       "      <td>R</td>\n",
       "      <td>Arizona_SOTS.txt</td>\n",
       "      <td>https://azgovernor.gov/governor/news/2019/01/g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Asa Hutchinson</td>\n",
       "      <td>R</td>\n",
       "      <td>Arkansas_SOTS.txt</td>\n",
       "      <td>https://governor.arkansas.gov/news-media/speec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>Gavin Newsom</td>\n",
       "      <td>D</td>\n",
       "      <td>California_SOTS.txt</td>\n",
       "      <td>https://www.gov.ca.gov/2019/02/12/state-of-the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state        governor party             filename  \\\n",
       "0     Alabama        Kay Ivey     R     Alabama_SOTS.txt   \n",
       "1      Alaska   Mike Dunleavy     R      Alaska_SOTS.txt   \n",
       "2     Arizona      Doug Ducey     R     Arizona_SOTS.txt   \n",
       "3    Arkansas  Asa Hutchinson     R    Arkansas_SOTS.txt   \n",
       "4  California    Gavin Newsom     D  California_SOTS.txt   \n",
       "\n",
       "                                                 url  \n",
       "0  https://governor.alabama.gov/remarks-speeches/...  \n",
       "1  https://gov.alaska.gov/newsroom/2019/01/22/201...  \n",
       "2  https://azgovernor.gov/governor/news/2019/01/g...  \n",
       "3  https://governor.arkansas.gov/news-media/speec...  \n",
       "4  https://www.gov.ca.gov/2019/02/12/state-of-the...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv for info on the data\n",
    "df = pd.read_csv(TABLE_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping filenames to text content\n",
    "speech_map = {}\n",
    "\n",
    "# read the files\n",
    "for fname in df['filename'].get_values():\n",
    "    with open( os.path.join(SPEECH_DIR, fname) ) as f:\n",
    "        speech_map[fname] = f.read()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/api/span#set_extension\n",
    "# extend Span object with is_stop\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# if the length is 1 and the word is a stopword\n",
    "stopword_checker_for_span = lambda span: (len(span) == 1 and type(span[0]).__name__ == \"Token\" and span[0].is_stop)\n",
    "Span.set_extension(\"is_stop\", getter=stopword_checker_for_span)\n",
    "\n",
    "# test ._.is_stop\n",
    "assert(nlp(\"I\")[:]._.is_stop == True)\n",
    "assert(nlp(\"Hello world\")[:]._.is_stop == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got noun chunks for Wyoming_SOTS.txt                \n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/usage/linguistic-features#noun-chunks\n",
    "\n",
    "speech_noun_map = {}\n",
    "\n",
    "for state in speech_map:\n",
    "    doc = nlp(speech_map[state])\n",
    "\n",
    "    nouns = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if (chunk._.is_stop): # e.g. if chunk == [\"it\"]\n",
    "            if DEBUG: \n",
    "                print(\"stopword?\", chunk[0], chunk.text)\n",
    "            continue\n",
    "\n",
    "        if (chunk[0].is_stop):\n",
    "            if len(chunk) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                print(\"\\t...found weird chunk {{\", chunk, \"}} \", end=\"\") if DEBUG else None\n",
    "                chunk = chunk[1:]\n",
    "                print(f\"...fixed... [[ {chunk} ]]\") if DEBUG else None\n",
    "\n",
    "        nouns.append(chunk.text)\n",
    "\n",
    "    \n",
    "    sys.stdout.write( f\"got noun chunks for {state}          \\r\" )\n",
    "    sys.stdout.flush()\n",
    "    speech_noun_map[state] = nouns\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lieutenant Governor Ainsworth', 'Pro Tempore Marsh', 'Speaker McCutcheon', 'Speaker Pro Tempore Gaston', 'members', 'Alabama Legislature', 'Chief Justice Parker', 'justices', 'Alabama Supreme Court', 'fellow Alabamians', 'Mother Nature', 'â€™s', 'form', 'state', 'significant devastation', 'least 23 innocent lives', 'Young children', 'life', 'Mothers', 'Fathers', 'Friends', 'neighbors', 'times', 'good Lord', 'continued comfort', 'healing hands', 'special thanks', 'emergency responders', 'local law enforcement', 'moment', 'silence', 'others', 'uncertainty', 'tomorrow', 'absolute certainty', 'resiliency', 'people', 'Alabama', 'time', 'entire nation', 'good people', 'Lee County', 'feet', '200 years', 'statehood', 'men', 'women', 'Alabama', 'nation', 'country']\n"
     ]
    }
   ],
   "source": [
    "print(speech_noun_map['Alabama_SOTS.txt'][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_babelnet(chunk):\n",
    "    \"\"\"\n",
    "    given a chunk of words, do call to BabelNet API and get results\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'lemma' : chunk,\n",
    "        'searchLang' : 'EN',\n",
    "        'key'  : BABEL_API_KEY\n",
    "    }\n",
    "    \n",
    "    url = SERVICE_URL + '?' + urlencode(params) # format the request url\n",
    "    request = Request(url)\n",
    "    request.add_header('Accept-encoding', 'gzip') # ask for gzip response\n",
    "    response = urlopen(request)\n",
    "    \n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "        buf = response.read()\n",
    "        fname = chunk + \"_\" + str(datetime.date.today()) + \".gzip\"\n",
    "        save_babelnet_bytes_results(buf, fname)\n",
    "        return json.loads(gzip.decompress(buf))\n",
    "    else:\n",
    "        raise Exception(\"unexpected response from BabelNet API\")\n",
    "\n",
    "def save_babelnet_bytes_results(babel_res, filename):\n",
    "    \"\"\"\n",
    "    given BabelNet result, save to file\n",
    "    \"\"\"\n",
    "    filename = os.path.join(BABEL_RESPONSE_DIR, filename)\n",
    "    try:\n",
    "        with open( filename, 'ab' ) as f: # append\n",
    "            f.write(babel_res)\n",
    "            f.close()\n",
    "    except:\n",
    "        with open( filename, 'wb' ) as f: # write/create\n",
    "            f.write(babel_res)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama Supreme Court\n"
     ]
    }
   ],
   "source": [
    "babel_data = {}\n",
    "chunk = speech_noun_map['Alabama_SOTS.txt'][8]\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'bn:14875491n', 'pos': 'NOUN', 'source': 'BABELNET'}]\n"
     ]
    }
   ],
   "source": [
    "# GOT THE RESPONSE!! WOOHOO!\n",
    "json_response = query_babelnet(chunk)\n",
    "babel_data = json_response\n",
    "print(babel_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
