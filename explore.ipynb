{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/bin/pip\n",
      "\n",
      "/usr/bin/python\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/bin/python3\n",
      "\n",
      "/bin/sh: /Library/Frameworks/Python.framework/Versions/3.7/bin/spacy: Permission denied\n"
     ]
    }
   ],
   "source": [
    "!which pip # expect pip to be inside .../Versions/3.x/bin/pip\n",
    "!echo\n",
    "!which python\n",
    "!echo\n",
    "!which python3 # make sure python3 is a thing\n",
    "!echo\n",
    "!spacy # make sure spacy is a thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.1.6)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (7.0.8)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.16.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.32.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cool punctuations !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# import nltk # nltk did not tokenize as expected\n",
    "import spacy\n",
    "import string # for string.punctuation\n",
    "print(\"cool punctuations\", string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\n",
      "doc [('When', True), ('Sebastian', False), ('Thrun', False), ('started', False), ('working', False), ('on', True), ('self', False), ('-', False), ('driving', False), ('cars', False), ('at', True), ('Google', False), ('in', True), ('2007', False), (',', False), ('few', True), ('people', False), ('outside', False), ('of', True), ('the', True), ('company', False), ('took', False), ('him', True), ('seriously', False), ('.', False), ('“', False), ('I', True), ('can', True), ('tell', False), ('you', True), ('very', True), ('senior', False), ('CEOs', False), ('of', True), ('major', False), ('American', False), ('car', False), ('companies', False), ('would', True), ('shake', False), ('my', True), ('hand', False), ('and', True), ('turn', False), ('away', False), ('because', True), ('I', True), ('was', True), ('n’t', False), ('worth', False), ('talking', False), ('to', True), (',', False), ('”', False), ('said', False), ('Thrun', False), (',', False), ('in', True), ('an', True), ('interview', False), ('with', True), ('Recode', False), ('earlier', False), ('this', True), ('week', False), ('.', False)]\n",
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'be', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "# # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Process whole documents\n",
    "# text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "#         \"Google in 2007, few people outside of the company took him \"\n",
    "#         \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "#         \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "#         \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "#         \"this week.\")\n",
    "# doc = nlp(text)\n",
    "\n",
    "# tok = doc[0]\n",
    "\n",
    "# # https://spacy.io/api/token\n",
    "\n",
    "# print(tok)\n",
    "\n",
    "# print(\"doc\",[(token.text, token.is_stop) for token in doc if not token.is_stop or not token.is_punct])\n",
    "\n",
    "# # Analyze syntax\n",
    "# print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "# print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# # Find named entities, phrases and concepts\n",
    "# for entity in doc.ents:\n",
    "#     print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mfekadu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# needed for nltk.word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR:\t ['words.csv', 'speeches', 'README.md', 'index.csv']\n",
      "SPEECH_DIR[:2]:\t ['Minnesota_SOTS.txt', 'Indiana_SOTS.txt']\n",
      "TABLE_PATH:\t state-of-the-state/index.csv\n",
      "SPEECH_DIR:\t state-of-the-state/speeches\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"state-of-the-state\"\n",
    "FILENAME = \"index.csv\"\n",
    "SPEECH_DIR_NAME = \"speeches\"\n",
    "TABLE_PATH = os.path.join(DATA_DIR, FILENAME)\n",
    "SPEECH_DIR = os.path.join(DATA_DIR, SPEECH_DIR_NAME)\n",
    "print(\"DATA_DIR:\\t\", os.listdir(DATA_DIR))\n",
    "print(\"SPEECH_DIR[:2]:\\t\", os.listdir(SPEECH_DIR)[:2])\n",
    "print(\"TABLE_PATH:\\t\", TABLE_PATH)\n",
    "print(\"SPEECH_DIR:\\t\", SPEECH_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>governor</th>\n",
       "      <th>party</th>\n",
       "      <th>filename</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Kay Ivey</td>\n",
       "      <td>R</td>\n",
       "      <td>Alabama_SOTS.txt</td>\n",
       "      <td>https://governor.alabama.gov/remarks-speeches/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>Mike Dunleavy</td>\n",
       "      <td>R</td>\n",
       "      <td>Alaska_SOTS.txt</td>\n",
       "      <td>https://gov.alaska.gov/newsroom/2019/01/22/201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>Doug Ducey</td>\n",
       "      <td>R</td>\n",
       "      <td>Arizona_SOTS.txt</td>\n",
       "      <td>https://azgovernor.gov/governor/news/2019/01/g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>Asa Hutchinson</td>\n",
       "      <td>R</td>\n",
       "      <td>Arkansas_SOTS.txt</td>\n",
       "      <td>https://governor.arkansas.gov/news-media/speec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>Gavin Newsom</td>\n",
       "      <td>D</td>\n",
       "      <td>California_SOTS.txt</td>\n",
       "      <td>https://www.gov.ca.gov/2019/02/12/state-of-the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state        governor party             filename  \\\n",
       "0     Alabama        Kay Ivey     R     Alabama_SOTS.txt   \n",
       "1      Alaska   Mike Dunleavy     R      Alaska_SOTS.txt   \n",
       "2     Arizona      Doug Ducey     R     Arizona_SOTS.txt   \n",
       "3    Arkansas  Asa Hutchinson     R    Arkansas_SOTS.txt   \n",
       "4  California    Gavin Newsom     D  California_SOTS.txt   \n",
       "\n",
       "                                                 url  \n",
       "0  https://governor.alabama.gov/remarks-speeches/...  \n",
       "1  https://gov.alaska.gov/newsroom/2019/01/22/201...  \n",
       "2  https://azgovernor.gov/governor/news/2019/01/g...  \n",
       "3  https://governor.arkansas.gov/news-media/speec...  \n",
       "4  https://www.gov.ca.gov/2019/02/12/state-of-the...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the csv for info on the data\n",
    "df = pd.read_csv(TABLE_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary mapping filenames to text content\n",
    "speech_map = {}\n",
    "\n",
    "# read the files\n",
    "for fname in df['filename'].get_values():\n",
    "    with open( os.path.join(SPEECH_DIR, fname) ) as f:\n",
    "        speech_map[fname] = f.read()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokenize() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9118c2668eb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspeech_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTweetTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspeech_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     print([x for x in tokens if x == '.' or x == '?'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     print(tokens.remove(','))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tokenize() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "for state in speech_map:\n",
    "    tokens = nltk.word_tokenize(speech_map[state])\n",
    "    print(tokens[:50])\n",
    "#     print([x for x in tokens if x == '.' or x == '?'])\n",
    "#     print(tokens.remove(','))\n",
    "#     print(tokens[:5])\n",
    "    bigram_generator = nltk.bigrams(tokens)\n",
    "    bigram_list = list(bigram_generator)\n",
    "#     print(bigram_list[:5])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['1','2','3',',']\n",
    "[x for x in tokens if x == ',']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokens.remove(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', 'not', 'ca', \"n't\", 'apple-pie']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.word_tokenize(\"cannot can't apple-pie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
